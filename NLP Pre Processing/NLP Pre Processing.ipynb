{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pre Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Riddhik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"covid.txt\", \"r\")\n",
    "text=f.read()\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coronavirus', 'disease', '(', 'COVID-19', ')', 'is', 'an', 'infectious', 'disease', 'caused', 'by', 'a', 'newly', 'discovered', 'coronavirus.Most', 'people', 'infected', 'with', 'the', 'COVID-19', 'virus', 'will', 'experience', 'mild', 'to', 'moderate', 'respiratory', 'illness', 'and', 'recover', 'without', 'requiring', 'special', 'treatment', '.', 'Older', 'people', ',', 'and', 'those', 'with', 'underlying', 'medical', 'problems', 'like', 'cardiovascular', 'disease', ',', 'diabetes', ',', 'chronic', 'respiratory', 'disease', ',', 'and', 'cancer', 'are', 'more', 'likely', 'to', 'develop', 'serious', 'illness.The', 'best', 'way', 'to', 'prevent', 'and', 'slow', 'down', 'transmission', 'is', 'to', 'be', 'well', 'informed', 'about', 'the', 'COVID-19', 'virus', ',', 'the', 'disease', 'it', 'causes', 'and', 'how', 'it', 'spreads', '.', 'Protect', 'yourself', 'and', 'others', 'from', 'infection', 'by', 'washing', 'your', 'hands', 'or', 'using', 'an', 'alcohol', 'based', 'rub', 'frequently', 'and', 'not', 'touching', 'your', 'face', '.', 'The', 'COVID-19', 'virus', 'spreads', 'primarily', 'through', 'droplets', 'of', 'saliva', 'or', 'discharge', 'from', 'the', 'nose', 'when', 'an', 'infected', 'person', 'coughs', 'or', 'sneezes', ',', 'so', 'it', 'is', 'important', 'that', 'you', 'also', 'practice', 'respiratory', 'etiquette', '(', 'for', 'example', ',', 'by', 'coughing', 'into', 'a', 'flexed', 'elbow', ')', '.'] \n",
      "\n",
      "After Removing Stop Words:\n",
      " ['Coronavirus', 'disease', '(', 'COVID-19', ')', 'infectious', 'disease', 'caused', 'newly', 'discovered', 'coronavirus.Most', 'people', 'infected', 'COVID-19', 'virus', 'experience', 'mild', 'moderate', 'respiratory', 'illness', 'recover', 'without', 'requiring', 'special', 'treatment', '.', 'Older', 'people', ',', 'underlying', 'medical', 'problems', 'like', 'cardiovascular', 'disease', ',', 'diabetes', ',', 'chronic', 'respiratory', 'disease', ',', 'cancer', 'likely', 'develop', 'serious', 'illness.The', 'best', 'way', 'prevent', 'slow', 'transmission', 'well', 'informed', 'COVID-19', 'virus', ',', 'disease', 'causes', 'spreads', '.', 'Protect', 'others', 'infection', 'washing', 'hands', 'using', 'alcohol', 'based', 'rub', 'frequently', 'touching', 'face', '.', 'The', 'COVID-19', 'virus', 'spreads', 'primarily', 'droplets', 'saliva', 'discharge', 'nose', 'infected', 'person', 'coughs', 'sneezes', ',', 'important', 'also', 'practice', 'respiratory', 'etiquette', '(', 'example', ',', 'coughing', 'flexed', 'elbow', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))  \n",
    "  \n",
    "word_tokens = word_tokenize(text)  \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "  \n",
    "filtered_sentence = []  \n",
    "  \n",
    "for w in word_tokens:  \n",
    "    if w not in stop_words:  \n",
    "        filtered_sentence.append(w)  \n",
    "  \n",
    "print(word_tokens,\"\\n\")  \n",
    "print(\"After Removing Stop Words:\\n\",filtered_sentence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus  :  coronaviru\n",
      "disease  :  diseas\n",
      "(  :  (\n",
      "COVID-19  :  covid-19\n",
      ")  :  )\n",
      "infectious  :  infecti\n",
      "disease  :  diseas\n",
      "caused  :  caus\n",
      "newly  :  newli\n",
      "discovered  :  discov\n",
      "coronavirus.Most  :  coronavirus.most\n",
      "people  :  peopl\n",
      "infected  :  infect\n",
      "COVID-19  :  covid-19\n",
      "virus  :  viru\n",
      "experience  :  experi\n",
      "mild  :  mild\n",
      "moderate  :  moder\n",
      "respiratory  :  respiratori\n",
      "illness  :  ill\n",
      "recover  :  recov\n",
      "without  :  without\n",
      "requiring  :  requir\n",
      "special  :  special\n",
      "treatment  :  treatment\n",
      ".  :  .\n",
      "Older  :  older\n",
      "people  :  peopl\n",
      ",  :  ,\n",
      "underlying  :  underli\n",
      "medical  :  medic\n",
      "problems  :  problem\n",
      "like  :  like\n",
      "cardiovascular  :  cardiovascular\n",
      "disease  :  diseas\n",
      ",  :  ,\n",
      "diabetes  :  diabet\n",
      ",  :  ,\n",
      "chronic  :  chronic\n",
      "respiratory  :  respiratori\n",
      "disease  :  diseas\n",
      ",  :  ,\n",
      "cancer  :  cancer\n",
      "likely  :  like\n",
      "develop  :  develop\n",
      "serious  :  seriou\n",
      "illness.The  :  illness.th\n",
      "best  :  best\n",
      "way  :  way\n",
      "prevent  :  prevent\n",
      "slow  :  slow\n",
      "transmission  :  transmiss\n",
      "well  :  well\n",
      "informed  :  inform\n",
      "COVID-19  :  covid-19\n",
      "virus  :  viru\n",
      ",  :  ,\n",
      "disease  :  diseas\n",
      "causes  :  caus\n",
      "spreads  :  spread\n",
      ".  :  .\n",
      "Protect  :  protect\n",
      "others  :  other\n",
      "infection  :  infect\n",
      "washing  :  wash\n",
      "hands  :  hand\n",
      "using  :  use\n",
      "alcohol  :  alcohol\n",
      "based  :  base\n",
      "rub  :  rub\n",
      "frequently  :  frequent\n",
      "touching  :  touch\n",
      "face  :  face\n",
      ".  :  .\n",
      "The  :  the\n",
      "COVID-19  :  covid-19\n",
      "virus  :  viru\n",
      "spreads  :  spread\n",
      "primarily  :  primarili\n",
      "droplets  :  droplet\n",
      "saliva  :  saliva\n",
      "discharge  :  discharg\n",
      "nose  :  nose\n",
      "infected  :  infect\n",
      "person  :  person\n",
      "coughs  :  cough\n",
      "sneezes  :  sneez\n",
      ",  :  ,\n",
      "important  :  import\n",
      "also  :  also\n",
      "practice  :  practic\n",
      "respiratory  :  respiratori\n",
      "etiquette  :  etiquett\n",
      "(  :  (\n",
      "example  :  exampl\n",
      ",  :  ,\n",
      "coughing  :  cough\n",
      "flexed  :  flex\n",
      "elbow  :  elbow\n",
      ")  :  )\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "for w in filtered_sentence: \n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus  :  Coronavirus\n",
      "disease  :  disease\n",
      "(  :  (\n",
      "COVID-19  :  COVID-19\n",
      ")  :  )\n",
      "infectious  :  infectious\n",
      "disease  :  disease\n",
      "caused  :  caused\n",
      "newly  :  newly\n",
      "discovered  :  discovered\n",
      "coronavirus.Most  :  coronavirus.Most\n",
      "people  :  people\n",
      "infected  :  infected\n",
      "COVID-19  :  COVID-19\n",
      "virus  :  virus\n",
      "experience  :  experience\n",
      "mild  :  mild\n",
      "moderate  :  moderate\n",
      "respiratory  :  respiratory\n",
      "illness  :  illness\n",
      "recover  :  recover\n",
      "without  :  without\n",
      "requiring  :  requiring\n",
      "special  :  special\n",
      "treatment  :  treatment\n",
      ".  :  .\n",
      "Older  :  Older\n",
      "people  :  people\n",
      ",  :  ,\n",
      "underlying  :  underlying\n",
      "medical  :  medical\n",
      "problems  :  problem\n",
      "like  :  like\n",
      "cardiovascular  :  cardiovascular\n",
      "disease  :  disease\n",
      ",  :  ,\n",
      "diabetes  :  diabetes\n",
      ",  :  ,\n",
      "chronic  :  chronic\n",
      "respiratory  :  respiratory\n",
      "disease  :  disease\n",
      ",  :  ,\n",
      "cancer  :  cancer\n",
      "likely  :  likely\n",
      "develop  :  develop\n",
      "serious  :  serious\n",
      "illness.The  :  illness.The\n",
      "best  :  best\n",
      "way  :  way\n",
      "prevent  :  prevent\n",
      "slow  :  slow\n",
      "transmission  :  transmission\n",
      "well  :  well\n",
      "informed  :  informed\n",
      "COVID-19  :  COVID-19\n",
      "virus  :  virus\n",
      ",  :  ,\n",
      "disease  :  disease\n",
      "causes  :  cause\n",
      "spreads  :  spread\n",
      ".  :  .\n",
      "Protect  :  Protect\n",
      "others  :  others\n",
      "infection  :  infection\n",
      "washing  :  washing\n",
      "hands  :  hand\n",
      "using  :  using\n",
      "alcohol  :  alcohol\n",
      "based  :  based\n",
      "rub  :  rub\n",
      "frequently  :  frequently\n",
      "touching  :  touching\n",
      "face  :  face\n",
      ".  :  .\n",
      "The  :  The\n",
      "COVID-19  :  COVID-19\n",
      "virus  :  virus\n",
      "spreads  :  spread\n",
      "primarily  :  primarily\n",
      "droplets  :  droplet\n",
      "saliva  :  saliva\n",
      "discharge  :  discharge\n",
      "nose  :  nose\n",
      "infected  :  infected\n",
      "person  :  person\n",
      "coughs  :  cough\n",
      "sneezes  :  sneeze\n",
      ",  :  ,\n",
      "important  :  important\n",
      "also  :  also\n",
      "practice  :  practice\n",
      "respiratory  :  respiratory\n",
      "etiquette  :  etiquette\n",
      "(  :  (\n",
      "example  :  example\n",
      ",  :  ,\n",
      "coughing  :  coughing\n",
      "flexed  :  flexed\n",
      "elbow  :  elbow\n",
      ")  :  )\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "for i in filtered_sentence:\n",
    "    print(i, \" : \",lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Coronavirus', 'NNP'), ('disease', 'NN'), ('(', '('), ('COVID-19', 'NNP'), (')', ')'), ('infectious', 'JJ'), ('disease', 'NN'), ('caused', 'VBD'), ('newly', 'RB'), ('discovered', 'VBN'), ('coronavirus.Most', 'NN'), ('people', 'NNS'), ('infected', 'VBD'), ('COVID-19', 'JJ'), ('virus', 'NN'), ('experience', 'NN'), ('mild', 'JJ'), ('moderate', 'JJ'), ('respiratory', 'NN'), ('illness', 'NN'), ('recover', 'NN'), ('without', 'IN'), ('requiring', 'VBG'), ('special', 'JJ'), ('treatment', 'NN'), ('.', '.'), ('Older', 'JJR'), ('people', 'NNS'), (',', ','), ('underlying', 'VBG'), ('medical', 'JJ'), ('problems', 'NNS'), ('like', 'IN'), ('cardiovascular', 'JJ'), ('disease', 'NN'), (',', ','), ('diabetes', 'VBZ'), (',', ','), ('chronic', 'JJ'), ('respiratory', 'NN'), ('disease', 'NN'), (',', ','), ('cancer', 'NN'), ('likely', 'RB'), ('develop', 'VBP'), ('serious', 'JJ'), ('illness.The', 'NN'), ('best', 'JJS'), ('way', 'NN'), ('prevent', 'NN'), ('slow', 'JJ'), ('transmission', 'NN'), ('well', 'RB'), ('informed', 'VBN'), ('COVID-19', 'NNP'), ('virus', 'NN'), (',', ','), ('disease', 'NN'), ('causes', 'NNS'), ('spreads', 'NNS'), ('.', '.'), ('Protect', 'NNP'), ('others', 'NNS'), ('infection', 'VBP'), ('washing', 'VBG'), ('hands', 'NNS'), ('using', 'VBG'), ('alcohol', 'NN'), ('based', 'VBN'), ('rub', 'NN'), ('frequently', 'RB'), ('touching', 'VBG'), ('face', 'NN'), ('.', '.'), ('The', 'DT'), ('COVID-19', 'JJ'), ('virus', 'NN'), ('spreads', 'NNS'), ('primarily', 'RB'), ('droplets', 'NNS'), ('saliva', 'VBP'), ('discharge', 'NN'), ('nose', 'RB'), ('infected', 'JJ'), ('person', 'NN'), ('coughs', 'NNS'), ('sneezes', 'VBZ'), (',', ','), ('important', 'JJ'), ('also', 'RB'), ('practice', 'NN'), ('respiratory', 'NN'), ('etiquette', 'NN'), ('(', '('), ('example', 'NN'), (',', ','), ('coughing', 'VBG'), ('flexed', 'JJ'), ('elbow', 'NN'), (')', ')'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(filtered_sentence) \n",
    "print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np \n",
    "import nltk\n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'the',\n",
       " 'disease',\n",
       " 'covid',\n",
       " '19',\n",
       " 'to',\n",
       " 'is',\n",
       " 'an',\n",
       " 'by',\n",
       " 'virus',\n",
       " 'respiratory',\n",
       " 'it',\n",
       " 'or',\n",
       " 'coronavirus',\n",
       " 'a',\n",
       " 'people',\n",
       " 'infected',\n",
       " 'with',\n",
       " 'illness',\n",
       " 'spreads',\n",
       " 'from',\n",
       " 'your',\n",
       " 'infectious',\n",
       " 'caused',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'most',\n",
       " 'will',\n",
       " 'experience',\n",
       " 'mild',\n",
       " 'moderate',\n",
       " 'recover',\n",
       " 'without',\n",
       " 'requiring',\n",
       " 'special',\n",
       " 'treatment',\n",
       " 'older',\n",
       " 'those',\n",
       " 'underlying',\n",
       " 'medical',\n",
       " 'problems',\n",
       " 'like',\n",
       " 'cardiovascular',\n",
       " 'diabetes',\n",
       " 'chronic',\n",
       " 'cancer',\n",
       " 'are',\n",
       " 'more',\n",
       " 'likely',\n",
       " 'develop',\n",
       " 'serious',\n",
       " 'best',\n",
       " 'way',\n",
       " 'prevent',\n",
       " 'slow',\n",
       " 'down',\n",
       " 'transmission',\n",
       " 'be',\n",
       " 'well',\n",
       " 'informed',\n",
       " 'about',\n",
       " 'causes',\n",
       " 'how',\n",
       " 'protect',\n",
       " 'yourself',\n",
       " 'others',\n",
       " 'infection',\n",
       " 'washing',\n",
       " 'hands',\n",
       " 'using',\n",
       " 'alcohol',\n",
       " 'based',\n",
       " 'rub',\n",
       " 'frequently',\n",
       " 'not',\n",
       " 'touching',\n",
       " 'face',\n",
       " 'primarily',\n",
       " 'through',\n",
       " 'droplets',\n",
       " 'of',\n",
       " 'saliva',\n",
       " 'discharge',\n",
       " 'nose',\n",
       " 'when',\n",
       " 'person',\n",
       " 'coughs',\n",
       " 'sneezes',\n",
       " 'so',\n",
       " 'important',\n",
       " 'that',\n",
       " 'you',\n",
       " 'also',\n",
       " 'practice',\n",
       " 'etiquette',\n",
       " 'for',\n",
       " 'example',\n",
       " 'coughing',\n",
       " 'into',\n",
       " 'flexed']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "for i in range(4):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text),i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coronavirus', 'disease', '(')\n",
      "('disease', '(', 'COVID-19')\n",
      "('(', 'COVID-19', ')')\n",
      "('COVID-19', ')', 'is')\n",
      "(')', 'is', 'an')\n",
      "('is', 'an', 'infectious')\n",
      "('an', 'infectious', 'disease')\n",
      "('infectious', 'disease', 'caused')\n",
      "('disease', 'caused', 'by')\n",
      "('caused', 'by', 'a')\n",
      "('by', 'a', 'newly')\n",
      "('a', 'newly', 'discovered')\n",
      "('newly', 'discovered', 'coronavirus.Most')\n",
      "('discovered', 'coronavirus.Most', 'people')\n",
      "('coronavirus.Most', 'people', 'infected')\n",
      "('people', 'infected', 'with')\n",
      "('infected', 'with', 'the')\n",
      "('with', 'the', 'COVID-19')\n",
      "('the', 'COVID-19', 'virus')\n",
      "('COVID-19', 'virus', 'will')\n",
      "('virus', 'will', 'experience')\n",
      "('will', 'experience', 'mild')\n",
      "('experience', 'mild', 'to')\n",
      "('mild', 'to', 'moderate')\n",
      "('to', 'moderate', 'respiratory')\n",
      "('moderate', 'respiratory', 'illness')\n",
      "('respiratory', 'illness', 'and')\n",
      "('illness', 'and', 'recover')\n",
      "('and', 'recover', 'without')\n",
      "('recover', 'without', 'requiring')\n",
      "('without', 'requiring', 'special')\n",
      "('requiring', 'special', 'treatment')\n",
      "('special', 'treatment', '.')\n",
      "('treatment', '.', 'Older')\n",
      "('.', 'Older', 'people')\n",
      "('Older', 'people', ',')\n",
      "('people', ',', 'and')\n",
      "(',', 'and', 'those')\n",
      "('and', 'those', 'with')\n",
      "('those', 'with', 'underlying')\n",
      "('with', 'underlying', 'medical')\n",
      "('underlying', 'medical', 'problems')\n",
      "('medical', 'problems', 'like')\n",
      "('problems', 'like', 'cardiovascular')\n",
      "('like', 'cardiovascular', 'disease')\n",
      "('cardiovascular', 'disease', ',')\n",
      "('disease', ',', 'diabetes')\n",
      "(',', 'diabetes', ',')\n",
      "('diabetes', ',', 'chronic')\n",
      "(',', 'chronic', 'respiratory')\n",
      "('chronic', 'respiratory', 'disease')\n",
      "('respiratory', 'disease', ',')\n",
      "('disease', ',', 'and')\n",
      "(',', 'and', 'cancer')\n",
      "('and', 'cancer', 'are')\n",
      "('cancer', 'are', 'more')\n",
      "('are', 'more', 'likely')\n",
      "('more', 'likely', 'to')\n",
      "('likely', 'to', 'develop')\n",
      "('to', 'develop', 'serious')\n",
      "('develop', 'serious', 'illness.The')\n",
      "('serious', 'illness.The', 'best')\n",
      "('illness.The', 'best', 'way')\n",
      "('best', 'way', 'to')\n",
      "('way', 'to', 'prevent')\n",
      "('to', 'prevent', 'and')\n",
      "('prevent', 'and', 'slow')\n",
      "('and', 'slow', 'down')\n",
      "('slow', 'down', 'transmission')\n",
      "('down', 'transmission', 'is')\n",
      "('transmission', 'is', 'to')\n",
      "('is', 'to', 'be')\n",
      "('to', 'be', 'well')\n",
      "('be', 'well', 'informed')\n",
      "('well', 'informed', 'about')\n",
      "('informed', 'about', 'the')\n",
      "('about', 'the', 'COVID-19')\n",
      "('the', 'COVID-19', 'virus')\n",
      "('COVID-19', 'virus', ',')\n",
      "('virus', ',', 'the')\n",
      "(',', 'the', 'disease')\n",
      "('the', 'disease', 'it')\n",
      "('disease', 'it', 'causes')\n",
      "('it', 'causes', 'and')\n",
      "('causes', 'and', 'how')\n",
      "('and', 'how', 'it')\n",
      "('how', 'it', 'spreads')\n",
      "('it', 'spreads', '.')\n",
      "('spreads', '.', 'Protect')\n",
      "('.', 'Protect', 'yourself')\n",
      "('Protect', 'yourself', 'and')\n",
      "('yourself', 'and', 'others')\n",
      "('and', 'others', 'from')\n",
      "('others', 'from', 'infection')\n",
      "('from', 'infection', 'by')\n",
      "('infection', 'by', 'washing')\n",
      "('by', 'washing', 'your')\n",
      "('washing', 'your', 'hands')\n",
      "('your', 'hands', 'or')\n",
      "('hands', 'or', 'using')\n",
      "('or', 'using', 'an')\n",
      "('using', 'an', 'alcohol')\n",
      "('an', 'alcohol', 'based')\n",
      "('alcohol', 'based', 'rub')\n",
      "('based', 'rub', 'frequently')\n",
      "('rub', 'frequently', 'and')\n",
      "('frequently', 'and', 'not')\n",
      "('and', 'not', 'touching')\n",
      "('not', 'touching', 'your')\n",
      "('touching', 'your', 'face')\n",
      "('your', 'face', '.')\n",
      "('face', '.', 'The')\n",
      "('.', 'The', 'COVID-19')\n",
      "('The', 'COVID-19', 'virus')\n",
      "('COVID-19', 'virus', 'spreads')\n",
      "('virus', 'spreads', 'primarily')\n",
      "('spreads', 'primarily', 'through')\n",
      "('primarily', 'through', 'droplets')\n",
      "('through', 'droplets', 'of')\n",
      "('droplets', 'of', 'saliva')\n",
      "('of', 'saliva', 'or')\n",
      "('saliva', 'or', 'discharge')\n",
      "('or', 'discharge', 'from')\n",
      "('discharge', 'from', 'the')\n",
      "('from', 'the', 'nose')\n",
      "('the', 'nose', 'when')\n",
      "('nose', 'when', 'an')\n",
      "('when', 'an', 'infected')\n",
      "('an', 'infected', 'person')\n",
      "('infected', 'person', 'coughs')\n",
      "('person', 'coughs', 'or')\n",
      "('coughs', 'or', 'sneezes')\n",
      "('or', 'sneezes', ',')\n",
      "('sneezes', ',', 'so')\n",
      "(',', 'so', 'it')\n",
      "('so', 'it', 'is')\n",
      "('it', 'is', 'important')\n",
      "('is', 'important', 'that')\n",
      "('important', 'that', 'you')\n",
      "('that', 'you', 'also')\n",
      "('you', 'also', 'practice')\n",
      "('also', 'practice', 'respiratory')\n",
      "('practice', 'respiratory', 'etiquette')\n",
      "('respiratory', 'etiquette', '(')\n",
      "('etiquette', '(', 'for')\n",
      "('(', 'for', 'example')\n",
      "('for', 'example', ',')\n",
      "('example', ',', 'by')\n",
      "(',', 'by', 'coughing')\n",
      "('by', 'coughing', 'into')\n",
      "('coughing', 'into', 'a')\n",
      "('into', 'a', 'flexed')\n",
      "('a', 'flexed', 'elbow')\n",
      "('flexed', 'elbow', ')')\n",
      "('elbow', ')', '.')\n"
     ]
    }
   ],
   "source": [
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 106 samples and 157 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word_tokens)\n",
    "print(fdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 7),\n",
       " (',', 7),\n",
       " ('disease', 5),\n",
       " ('COVID-19', 4),\n",
       " ('the', 4),\n",
       " ('to', 4),\n",
       " ('.', 4),\n",
       " ('is', 3),\n",
       " ('an', 3),\n",
       " ('by', 3),\n",
       " ('virus', 3),\n",
       " ('respiratory', 3),\n",
       " ('it', 3),\n",
       " ('or', 3),\n",
       " ('(', 2),\n",
       " (')', 2),\n",
       " ('a', 2),\n",
       " ('people', 2),\n",
       " ('infected', 2),\n",
       " ('with', 2),\n",
       " ('spreads', 2),\n",
       " ('from', 2),\n",
       " ('your', 2),\n",
       " ('Coronavirus', 1),\n",
       " ('infectious', 1),\n",
       " ('caused', 1),\n",
       " ('newly', 1),\n",
       " ('discovered', 1),\n",
       " ('coronavirus.Most', 1),\n",
       " ('will', 1),\n",
       " ('experience', 1),\n",
       " ('mild', 1),\n",
       " ('moderate', 1),\n",
       " ('illness', 1),\n",
       " ('recover', 1),\n",
       " ('without', 1),\n",
       " ('requiring', 1),\n",
       " ('special', 1),\n",
       " ('treatment', 1),\n",
       " ('Older', 1),\n",
       " ('those', 1),\n",
       " ('underlying', 1),\n",
       " ('medical', 1),\n",
       " ('problems', 1),\n",
       " ('like', 1),\n",
       " ('cardiovascular', 1),\n",
       " ('diabetes', 1),\n",
       " ('chronic', 1),\n",
       " ('cancer', 1),\n",
       " ('are', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
